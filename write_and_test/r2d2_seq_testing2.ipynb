{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "31e49c43-0769-493a-b7a1-e0a659843274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../r2d2_algo/')\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import torch\n",
    "from torch import nn\n",
    "from segment_tree import SumSegmentTree, MinSegmentTree\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from model import RNNQNetwork, linear_schedule\n",
    "from storage import ContinuousSequenceReplayBuffer, SequenceReplayBuffer\n",
    "from envs import make_vec_envs\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_nav\n",
    "import time\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_action_dim(action_space):\n",
    "    \"\"\"\n",
    "    Get the dimension of the action space.\n",
    "    \"\"\"\n",
    "    if isinstance(action_space, spaces.Box):\n",
    "        return int(np.prod(action_space.shape))\n",
    "    elif isinstance(action_space, spaces.Discrete):\n",
    "        # Action is an int\n",
    "        return 1\n",
    "    elif isinstance(action_space, spaces.MultiDiscrete):\n",
    "        # Number of discrete actions\n",
    "        return int(len(action_space.nvec))\n",
    "    elif isinstance(action_space, spaces.MultiBinary):\n",
    "        # Number of binary actions\n",
    "        assert isinstance(\n",
    "            action_space.n, int\n",
    "        ), \"Multi-dimensional MultiBinary action space is not supported. You can flatten it instead.\"\n",
    "        return int(action_space.n)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{action_space} action space is not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "dcf8aacd-89cf-49ba-806f-a59b7fad6637",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class SequenceReplayBuffer:\n",
    "    def __init__(self, buffer_size, observation_space,\n",
    "                 action_space, hidden_state_size, sequence_length=8,\n",
    "                 burn_in_length=4, n_envs=1,\n",
    "                 alpha=0.6, beta=0.4, \n",
    "                 beta_increment=0.0001, max_priority=1.0):\n",
    "        '''\n",
    "        A replay buffer for R2D2 algorithm that when sampled, produces sequences of time steps.\n",
    "        Will continually store steps until a done is received or max sequence length is reached\n",
    "            then store that sequence into the replay buffer\n",
    "          \n",
    "        self.pos keeps track of the next index to be written to. When it reaches the end \n",
    "          it loops back to the start.\n",
    "        \n",
    "        buffer_size: number of sequences to hold in buffer\n",
    "        sequence_length: number of steps in sequence\n",
    "        burn_in_length: number of steps before idx to be passed with sequence\n",
    "        '''\n",
    "        self.buffer_size = buffer_size\n",
    "        self.bil_sl = burn_in_length + sequence_length\n",
    "        total_buffer_size = buffer_size + sequence_length + burn_in_length\n",
    "        self.n_envs = n_envs\n",
    "        self.sequence_length = sequence_length\n",
    "        self.burn_in_length = burn_in_length\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_increment = beta_increment\n",
    "        self.max_priority = max_priority\n",
    "\n",
    "        # capacity must be positive and a power of 2.\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < buffer_size:\n",
    "            tree_capacity *= 2\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity) #trees hold actual priorities for faster updating and sampling\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "        \n",
    "        # buffer shape [buffer_size, sequence_length, data_dim]\n",
    "        #  note that we add to the buffer regardless of which env the sequence comes from\n",
    "        \n",
    "        action_shape = get_action_dim(action_space)\n",
    "        self.observations = np.zeros((buffer_size, self.bil_sl, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.next_observations = np.zeros((buffer_size, self.bil_sl, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.actions = np.zeros((buffer_size, self.bil_sl, action_shape), dtype=action_space.dtype)\n",
    "        self.rewards = np.zeros((buffer_size, self.bil_sl), dtype=np.float32)\n",
    "        self.dones = np.zeros((buffer_size, self.bil_sl), dtype=np.float32)\n",
    "        self.hidden_states = np.zeros((buffer_size, self.bil_sl, hidden_state_size), dtype=np.float32)\n",
    "        self.next_hidden_states = np.zeros((buffer_size, self.bil_sl, hidden_state_size), dtype=np.float32)\n",
    "        # training_masks is used to keep track of which steps are trainable\n",
    "        #  note that it only has length sequence_length, as opposed to bil+sl\n",
    "        self.training_masks = np.zeros((buffer_size, self.sequence_length), dtype=np.float32)\n",
    "\n",
    "        self.cur_observations = np.zeros((n_envs, self.bil_sl, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.cur_next_observations = np.zeros((n_envs, self.bil_sl, *observation_space.shape), dtype=observation_space.dtype)\n",
    "        self.cur_actions = np.zeros((n_envs, self.bil_sl, action_shape), dtype=action_space.dtype)\n",
    "        self.cur_rewards = np.zeros((n_envs, self.bil_sl), dtype=np.float32)\n",
    "        self.cur_dones = np.zeros((n_envs, self.bil_sl), dtype=np.float32)\n",
    "        self.cur_hidden_states = np.zeros((n_envs, self.bil_sl, hidden_state_size), dtype=np.float32)\n",
    "        self.cur_next_hidden_states = np.zeros((n_envs, self.bil_sl, hidden_state_size), dtype=np.float32)\n",
    "\n",
    "        self.pos = 0\n",
    "        \n",
    "        self.cur_pos = np.zeros(n_envs, dtype='long') # keep track of which environments are done\n",
    "        self.full = False\n",
    "        \n",
    "        \n",
    "    def add(self, obs, next_obs, action, reward, done, hidden_state, next_hidden_state):\n",
    "        '''\n",
    "        Add to the buffer. Each incoming input should be of shape\n",
    "            [n_envs, data_dim]\n",
    "        '''\n",
    "        for i in range(self.n_envs):\n",
    "            self.cur_observations[i, self.cur_pos[i]] = np.array(obs[i]).copy()\n",
    "            self.cur_next_observations[i, self.cur_pos[i]] = np.array(next_obs[i]).copy()\n",
    "            self.cur_actions[i, self.cur_pos[i]] = np.array(action[i]).copy()\n",
    "            self.cur_rewards[i, self.cur_pos[i]] = np.array(reward[i]).copy()\n",
    "            self.cur_dones[i, self.cur_pos[i]] = np.array(done[i]).copy()\n",
    "            self.cur_hidden_states[i, self.cur_pos[i]] = np.array(hidden_state[:, i, :]).copy()\n",
    "            self.cur_next_hidden_states[i, self.cur_pos[i]] = np.array(next_hidden_state[:, i, :]).copy()\n",
    "            self.cur_pos[i] += 1\n",
    "\n",
    "            if done[i] or self.cur_pos[i] == self.bil_sl:\n",
    "                # copy the sequence to the buffer\n",
    "                self.observations[self.pos] = self.cur_observations[i]\n",
    "                self.next_observations[self.pos] = self.cur_next_observations[i]\n",
    "                self.actions[self.pos] = self.cur_actions[i]\n",
    "                self.rewards[self.pos] = self.cur_rewards[i]\n",
    "                self.dones[self.pos] = self.cur_dones[i]\n",
    "                self.hidden_states[self.pos] = self.cur_hidden_states[i]\n",
    "                self.next_hidden_states[self.pos] = self.cur_next_hidden_states[i]\n",
    "\n",
    "                trainable_steps = self.cur_pos[i] - self.burn_in_length\n",
    "                training_mask = np.zeros((self.sequence_length,))\n",
    "                training_mask[:trainable_steps] = 1\n",
    "                self.training_masks[self.pos] = training_mask\n",
    "                                \n",
    "                # make copy of the last steps to carry over to next sequence\n",
    "                copy_steps = min(self.cur_pos[i], self.burn_in_length)\n",
    "                \n",
    "                # only copy if not done. If done, start a new sequence up\n",
    "                if not done[i]:\n",
    "                    self.cur_observations[i, :copy_steps] = self.cur_observations[i, self.cur_pos[i]-copy_steps:self.cur_pos[i]]\n",
    "                    self.cur_next_observations[i, :copy_steps] = self.cur_next_observations[i, self.cur_pos[i]-copy_steps:self.cur_pos[i]]\n",
    "                    self.cur_actions[i, :copy_steps] = self.cur_actions[i, self.cur_pos[i]-copy_steps:self.cur_pos[i]]\n",
    "                    self.cur_rewards[i, :copy_steps] = self.cur_rewards[i, self.cur_pos[i]-copy_steps:self.cur_pos[i]]\n",
    "                    self.cur_dones[i, :copy_steps] = self.cur_dones[i, self.cur_pos[i]-copy_steps:self.cur_pos[i]]\n",
    "                    self.cur_hidden_states[i, :copy_steps] = self.cur_hidden_states[i, self.cur_pos[i]-copy_steps:self.cur_pos[i]]\n",
    "                    self.cur_next_hidden_states[i, :copy_steps] = self.cur_next_hidden_states[i, self.cur_pos[i]-copy_steps:self.cur_pos[i]]\n",
    "                else:\n",
    "                    copy_steps = 0\n",
    "                \n",
    "                self.cur_observations[i, copy_steps:] = 0.\n",
    "                self.cur_next_observations[i, copy_steps:] = 0.\n",
    "                self.cur_actions[i, copy_steps:] = 0.\n",
    "                self.cur_rewards[i, copy_steps:] = 0.\n",
    "                self.cur_dones[i, copy_steps:] = 0.\n",
    "                self.cur_hidden_states[i, copy_steps:] = 0.\n",
    "                self.cur_next_hidden_states[i, copy_steps:] = 0.\n",
    "                \n",
    "                self.cur_pos[i] = copy_steps\n",
    "                \n",
    "                self.sum_tree[self.pos] = self.max_priority ** self.alpha\n",
    "                self.min_tree[self.pos] = self.max_priority ** self.alpha\n",
    "                \n",
    "                self.pos = (self.pos + 1) % self.buffer_size\n",
    "\n",
    "                \n",
    "            \n",
    "    def _sample_proportional(self, num_sequences):\n",
    "        '''\n",
    "        Use sum tree to sample indices from priorities in segments\n",
    "        '''\n",
    "        indices = []\n",
    "        p_total = self.sum_tree.sum()\n",
    "        segment = p_total / num_sequences\n",
    "        \n",
    "        # Check if stratified sampling will be valid based on number\n",
    "        #  of sequences asked for and fullness of storage        \n",
    "        for i in range(num_sequences):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            \n",
    "            upperbound = random.uniform(a, b)\n",
    "            idx = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(idx)\n",
    "                    \n",
    "        return indices\n",
    "    \n",
    "    \n",
    "    def _sample_uniform(self, num_sequences=1):\n",
    "        '''\n",
    "        Use sum tree to get n sample indices without stratified sampling\n",
    "        '''\n",
    "        indices = []\n",
    "        p_total = self.sum_tree.sum()\n",
    "        total_trainable_steps = 0\n",
    "\n",
    "        for i in range(num_sequences):\n",
    "            upperbound = random.uniform(0, p_total)\n",
    "            idx = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(idx)\n",
    "            total_trainable_steps += (self.training_masks[idx].sum())\n",
    "\n",
    "        return indices, total_trainable_steps\n",
    "        \n",
    "        \n",
    "    def _calculate_weight(self, idx):\n",
    "        '''Calculate the weight of the experience at idx.'''\n",
    "        # print(t_idx, env_idx)\n",
    "        \n",
    "        size = self.buffer_size if self.full else self.pos\n",
    "        size = size * self.n_envs\n",
    "        \n",
    "        # get max weight\n",
    "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (p_min * size) ** (-self.beta)\n",
    "        \n",
    "        # calculate weights\n",
    "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
    "        weight = (p_sample * size) ** (-self.beta)\n",
    "        weight = weight / max_weight\n",
    "        \n",
    "        return weight\n",
    "        \n",
    "        \n",
    "    def sample(self, num_steps=None, num_sequences=None):\n",
    "        '''\n",
    "        Generate a sample, either by number of sequences or number of steps\n",
    "        Given a number of sequences, use stratified sampling\n",
    "        '''\n",
    "        if num_sequences is not None:\n",
    "            idxs = self._sample_indices(num_sequences)\n",
    "        elif num_steps is not None:\n",
    "            idxs = []\n",
    "            num_fails = 0\n",
    "            trainable_steps_batched = 0\n",
    "            while trainable_steps_batched < num_steps and num_fails < 50:\n",
    "                new_idxs, n_trainable_steps = self._sample_uniform(1)\n",
    "                idxs += new_idxs\n",
    "                if n_trainable_steps <= 0:\n",
    "                    num_fails += 1\n",
    "                else:\n",
    "                    trainable_steps_batched += n_trainable_steps\n",
    "            \n",
    "            if num_fails >= 50:\n",
    "                print('Warning - sampling failed 50 times')\n",
    "        else:\n",
    "            raise Exception('One of num_steps or num_sequences must be given')\n",
    "            \n",
    "        \n",
    "        # weights are [N, 1] tensor to be multiplied to each sequence batch generaated\n",
    "        weights = torch.Tensor([self._calculate_weight(idxs[i]) \\\n",
    "                                for i in range(len(idxs))]).reshape(-1, 1)\n",
    "\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "                \n",
    "        obs = torch.Tensor(self.observations[idxs])\n",
    "        next_obs = torch.Tensor(self.next_observations[idxs])\n",
    "        actions = torch.Tensor(self.actions[idxs])\n",
    "        rewards = torch.Tensor(self.rewards[idxs])\n",
    "        dones = torch.Tensor(self.dones[idxs])\n",
    "        next_dones = torch.Tensor(self.dones[idxs])\n",
    "        hidden_states = torch.Tensor(self.hidden_states[idxs, 0, :]).unsqueeze(0)\n",
    "        next_hidden_states = torch.Tensor(self.next_hidden_states[idxs, 0, :]).unsqueeze(0)\n",
    "        training_masks = torch.Tensor(self.training_masks[idxs])\n",
    "        \n",
    "        sample = {\n",
    "            'observations': obs,\n",
    "            'next_observations': next_obs,\n",
    "            'actions': actions,\n",
    "            'rewards': rewards,\n",
    "            'dones': dones,\n",
    "            'next_dones': next_dones,\n",
    "            'hidden_states': hidden_states,\n",
    "            'next_hidden_states': next_hidden_states,\n",
    "            'training_masks': training_masks,\n",
    "            'weights': weights,\n",
    "            'idxs': idxs,\n",
    "        }\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "\n",
    "    def update_priorities(self, idxs, priorities):\n",
    "        '''\n",
    "        idxs: shape [N,]\n",
    "        priorities: shape [N,]\n",
    "        '''\n",
    "        \n",
    "        assert len(idxs) == len(priorities)\n",
    "\n",
    "        for idx, priority in zip(idxs, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "\n",
    "            self.sum_tree[idx] = priority ** self.alpha\n",
    "            self.min_tree[idx] = priority ** self.alpha\n",
    "\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.full:\n",
    "            return self.buffer_size\n",
    "        else:\n",
    "            return self.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "08daedec-4060-45bc-9826-3549ea939e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2D2Agent(nn.Module):\n",
    "    def __init__(self, batch_size=128, burn_in_length=4, sequence_length=8,\n",
    "                 gamma=0.99, tau=1., learning_rate=2.5e-4, hidden_size=64, adam_epsilon=1e-8,\n",
    "                 device=torch.device('cpu'), buffer_size=10_000, \n",
    "                 learning_starts=10_000, train_frequency=10, target_network_frequency=500,\n",
    "                 total_timesteps=30_000, start_e=1., end_e=0.05, exploration_fraction=0.5, \n",
    "                 alpha=0.6, beta=0.4,\n",
    "                 seed=None, n_envs=1, dummy_env=True,\n",
    "                 env_id='CartPole-v1', env_kwargs={},\n",
    "                 verbose=0, q_network=None,  deterministic=False, env=None,\n",
    "                 writer=None, handle_target_network=True):\n",
    "        \"\"\"\n",
    "        R2D2 setup following same parameters as args.py has\n",
    "        verbose: Level of verbosity of print statements\n",
    "            1: print episode lengths and returns means every 2000 steps\n",
    "            2: print every episode length and return\n",
    "        q_network: Mostly for use of evaluation with a saved q_network\n",
    "          optionally pass in a q_network to use manually\n",
    "        deterministic: If True, manually set epsilon to 0 for every act() call\n",
    "        env: Also option to manually pass in an environment\n",
    "        n_envs: option to make multiple envs and have q_network generate multiple\n",
    "        dummy_env: whether to use DummyVecEnv as opposed to SubprocVecEnv for testing\n",
    "        writer: option to pass a tensorboard SummaryWriter object\n",
    "        handle_target_network: whether this class is in charge of updating the target network\n",
    "            params\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.buffer_size = buffer_size\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.learning_starts = learning_starts\n",
    "        self.train_frequency = train_frequency\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.adam_epsilon = adam_epsilon\n",
    "        self.target_network_frequency = target_network_frequency\n",
    "        self.handle_target_network = handle_target_network\n",
    "        self.device = device\n",
    "\n",
    "        self.start_e = start_e\n",
    "        self.end_e = end_e\n",
    "        self.exploration_fraction = exploration_fraction\n",
    "\n",
    "        self.burn_in_length = burn_in_length\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.n_envs = n_envs\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.seed = seed\n",
    "        self.deterministic = deterministic\n",
    "        if env == None:\n",
    "            # self.env = gym.make(env_id, **env_kwargs)\n",
    "            self.env = make_vec_envs(env_id, n_envs, env_kwargs=env_kwargs,\n",
    "                                     dummy=dummy_env)\n",
    "        else:\n",
    "            self.env = env\n",
    "        \n",
    "        \n",
    "        if q_network == None:\n",
    "            self.q_network = RNNQNetwork(self.env, hidden_size).to(device)\n",
    "        else:\n",
    "            self.q_network = q_network\n",
    "        self.target_network = RNNQNetwork(self.env, hidden_size).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "        \n",
    "        # self.rb = ContinuousSequenceReplayBuffer(buffer_size, self.env.observation_space, self.env.action_space,\n",
    "        #                         hidden_size, sequence_length=sequence_length, \n",
    "        #                         burn_in_length=burn_in_length, n_envs=n_envs,\n",
    "        #                         alpha=alpha, beta=beta)\n",
    "        self.rb = SequenceReplayBuffer(buffer_size, self.env.observation_space, self.env.action_space,\n",
    "                                hidden_size, sequence_length=sequence_length, \n",
    "                                burn_in_length=burn_in_length, n_envs=n_envs,\n",
    "                                alpha=alpha, beta=beta)\n",
    "\n",
    "        \n",
    "        self.global_step = 0\n",
    "        self.global_update_step = 0\n",
    "        self.rnn_hxs = self.q_network.get_rnn_hxs(self.n_envs)\n",
    "        self.obs = self.env.reset()\n",
    "        self.masks = torch.zeros((self.n_envs, 1), dtype=torch.float32)\n",
    "        \n",
    "        self.cur_episode_t = np.zeros(self.n_envs)\n",
    "        self.cur_episode_r = np.zeros(self.n_envs)\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        self.writer = writer\n",
    "        self.start_time = time.time()\n",
    "        self.lengths = []\n",
    "        self.returns = []\n",
    "        \n",
    "    \n",
    "    def act(self, obs, rnn_hxs, use_epsilon=True, masks=None):\n",
    "        \"\"\"Compute q values and sample policy. If epsilon is True,\n",
    "        perform randomo action with probability based on current global timestep\n",
    "        \n",
    "        masks: tensor of shape (N, 1) which has entries 0.0 when done\n",
    "            and 1.0 when not done, indicating when rnn_hxs should be reset\n",
    "            Used for vectorized environments\n",
    "        \"\"\"            \n",
    "        epsilon = self.get_epsilon(use_epsilon)\n",
    "        \n",
    "        obs_tensor = torch.Tensor(obs).to(self.device)\n",
    "        if obs_tensor.dim() < rnn_hxs.dim():\n",
    "            # We have an observation from the environment but need to unsqueeze\n",
    "            #  to tell the GRU that this is an observation of time length 1\n",
    "            # If it is batched (dim == 2), then we add an axis in the middle\n",
    "            #  otherwise add it to the start\n",
    "            if obs_tensor.dim() == 1:\n",
    "                obs_tensor = obs_tensor.unsqueeze(0)\n",
    "                action_dim = 1\n",
    "            elif obs_tensor.dim() == 2:\n",
    "                obs_tensor = obs_tensor.unsqueeze(1)\n",
    "                action_dim = 2\n",
    "        \n",
    "        else:\n",
    "            if obs_tensor.dim() == 2:\n",
    "                action_dim = 1\n",
    "            elif obs_tensor.dim() == 3:\n",
    "                action_dim = 2  \n",
    "            \n",
    "        q_values, gru_out, next_rnn_hxs = self.q_network(obs_tensor, rnn_hxs, masks=masks)\n",
    "                \n",
    "        \n",
    "        # action = np.array([[q_values.argmax()]])\n",
    "        action = q_values.argmax(dim=action_dim).numpy()\n",
    "        if use_epsilon:\n",
    "            if len(action.shape) == 1:\n",
    "                for i in range(action.shape[0]):\n",
    "                    if random.random() < epsilon:\n",
    "                        action[i] = self.env.action_space.sample()\n",
    "            elif len(action.shape) == 2:\n",
    "                for i in range(action.shape[0]):\n",
    "                    for j in range(action.shape[1]):\n",
    "                        if random.random() < epsilon:\n",
    "                            action[i, j] = self.env.action_space.sample()\n",
    "            \n",
    "        if len(action.shape) == 1:\n",
    "            action = action[np.newaxis, :]\n",
    "\n",
    "        return action, q_values, next_rnn_hxs\n",
    "                \n",
    "        \n",
    "    def collect(self, num_steps):\n",
    "        \"\"\"Perform policy for n steps and add to memory buffer\n",
    "        \n",
    "        Note that we will add a total of num_steps * self.n_envs to the buffer\"\"\"\n",
    "        env = self.env\n",
    "        \n",
    "        for t in range(num_steps):\n",
    "            action, q_values, next_rnn_hxs = self.act(self.obs, self.rnn_hxs, masks=self.masks)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            self.cur_episode_r += reward\n",
    "            self.cur_episode_t += 1\n",
    "            \n",
    "            # Masks are used to reset hidden state when vectorized environmnts give dones\n",
    "            self.masks = torch.FloatTensor(\n",
    "                [[0.0] if done_ else [1.0] for done_ in done])\n",
    "\n",
    "            for i, done_ in enumerate(done):\n",
    "                if done_:\n",
    "                    if self.verbose == 2:\n",
    "                        print(f'Episode R: {self.cur_episode_r[i]}, L: {self.cur_episode_t[i]}')\n",
    "                        \n",
    "                    if self.writer is not None:\n",
    "                        self.writer.add_scalar('charts/episodic_return', self.cur_episode_r[i], self.global_step)\n",
    "                        self.writer.add_scalar('charts/episodic_length', self.cur_episode_t[i], self.global_step)\n",
    "                        self.writer.add_scalar('charts/epsilon', self.get_epsilon(), self.global_step)\n",
    "\n",
    "                    self.lengths.append(self.cur_episode_t[i])\n",
    "                    self.returns.append(self.cur_episode_r[i])\n",
    "                    \n",
    "                    self.cur_episode_r[i] = 0\n",
    "                    self.cur_episode_t[i] = 0\n",
    "                    \n",
    "\n",
    "            # for ContinuousSequenceReplayBuffer\n",
    "            # self.rb.add(self.obs, next_obs, action, reward, done, self.rnn_hxs.detach())\n",
    "            self.rb.add(self.obs, next_obs, action, reward, done, self.rnn_hxs.detach(), next_rnn_hxs.detach())\n",
    "            \n",
    "            self.obs = next_obs\n",
    "            self.rnn_hxs = next_rnn_hxs\n",
    "            \n",
    "            self.global_step += self.n_envs\n",
    "            \n",
    "            if self.handle_target_network and self.global_step > self.learning_starts and \\\n",
    "                self.global_step % self.target_network_frequency < self.n_envs:\n",
    "                for target_network_param, q_network_param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
    "                    target_network_param.data.copy_(\n",
    "                        self.tau * q_network_param.data + (1 - self.tau) * target_network_param.data\n",
    "                    )\n",
    "            \n",
    "            if self.global_step % 2000 < self.n_envs:\n",
    "                if self.verbose == 1:\n",
    "                    print(f'Mean episode length {np.mean(self.lengths)}, mean return {np.mean(self.returns)}')\n",
    "                self.lengths = []\n",
    "                self.returns = []\n",
    "\n",
    "                \n",
    "            \n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Sample from buffer and perform Q-learning\"\"\"\n",
    "        \n",
    "        sample = self.rb.sample(self.batch_size//self.sequence_length)\n",
    "        states = sample['observations']\n",
    "        next_states = sample['next_observations']\n",
    "        hidden_states = sample['hidden_states']\n",
    "        next_hidden_states = sample['next_hidden_states']\n",
    "        actions = sample['actions']\n",
    "        rewards = sample['rewards']\n",
    "        dones = sample['dones']\n",
    "        next_dones = sample['next_dones']\n",
    "        # training masks come from SequenceReplayBuffer and tell us which\n",
    "        #  steps in each sequence actually are viable for training\n",
    "        training_masks = sample['training_masks']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_q, _, _ = self.target_network(next_states, next_hidden_states, next_dones)\n",
    "            target_max, _ = target_q.max(dim=2)\n",
    "            td_target = rewards + self.gamma * target_max * (1 - dones)\n",
    "        old_q, _, _ = self.q_network(states, hidden_states, dones)\n",
    "        old_val = old_q.gather(2, actions.long()).squeeze()\n",
    "\n",
    "        # loss = F.mse_loss(td_target[:, self.burn_in_length:], old_val[:, self.burn_in_length:])\n",
    "        weights = sample['weights']\n",
    "        elementwise_loss = F.smooth_l1_loss(td_target[:, self.burn_in_length:],\n",
    "                                            old_val[:, self.burn_in_length:], reduction='none')\n",
    "        # loss = torch.mean(elementwise_loss * weights)\n",
    "        loss = torch.mean(elementwise_loss * weights * training_masks)\n",
    "                \n",
    "        if self.writer is not None and self.global_update_step % 10 == 0:\n",
    "            self.writer.add_scalar('losses/td_loss', loss, self.global_step)\n",
    "            self.writer.add_scalar('losses/q_values', old_val.mean().item(), self.global_step)\n",
    "            sps = int(self.global_step / (time.time() - self.start_time))\n",
    "            # print('SPS:', int(sps))\n",
    "            self.writer.add_scalar('charts/SPS', sps, self.global_step)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # PER: update priorities\n",
    "        # for ContinuousSequenceReplayBuffer\n",
    "        # td_priorities = elementwise_loss.detach().cpu().numpy() + 1e-6\n",
    "        # self.rb.update_priorities(sample['seq_idxs'][:, self.burn_in_length:],\n",
    "        #                           sample['env_idxs'], td_priorities)\n",
    "        td_priorities = elementwise_loss.mean(dim=1).detach().cpu().numpy() + 1e-6\n",
    "        self.rb.update_priorities(sample['idxs'], td_priorities)\n",
    "\n",
    "        self.global_update_step += 1\n",
    "    \n",
    "\n",
    "    def train(self, n_updates):\n",
    "        if self.global_step < self.learning_starts:\n",
    "            self.collect((self.learning_starts - self.global_step) // self.n_envs + 1)\n",
    "        \n",
    "        for i in range(n_updates):\n",
    "            self.collect(self.train_frequency)\n",
    "            self.update()\n",
    "\n",
    "\n",
    "    def get_rnn_hxs(self):\n",
    "        return self.q_network.get_rnn_hxs(self.n_envs)\n",
    "    \n",
    "    def get_epsilon(self, use_epsilon=True):\n",
    "        if use_epsilon:\n",
    "            epsilon = linear_schedule(self.start_e, self.end_e, \n",
    "                        self.exploration_fraction*self.total_timesteps,\n",
    "                        self.global_step)\n",
    "        else:\n",
    "            epsilon = 0\n",
    "            \n",
    "        return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "2f217627-1c8d-494c-96b9-d990c236a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "        'num_objects': 0, 'rew_structure': 'goal',\n",
    "        'task_structure': 2, 'wall_colors': 4,\n",
    "        'num_rays': 12, 'fov': 1\n",
    "}\n",
    "env = gym.make('NavEnv-v0', **env_kwargs)\n",
    "agent = R2D2Agent(env_id='NavEnv-v0', env_kwargs=env_kwargs,\n",
    "                 verbose=1, buffer_size=5000, alpha=0.6, batch_size=256,\n",
    "                 burn_in_length=4, n_envs=4, dummy_env=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "82f364b3-815c-42e6-b74f-0008a16a670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean episode length 202.0, mean return 0.0\n",
      "Mean episode length 202.0, mean return 0.0\n"
     ]
    }
   ],
   "source": [
    "agent.collect(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "86c99ff5-040b-4523-80cf-7bc58c8437d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = agent.rb.sample(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a90d4066-1c8c-471d-a7b4-23aa8bd6f9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37, 16, 24])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['observations'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dd0af12b-9f33-4abb-bfae-1afc4c545727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "action, q_values, next_rnn_hxs = agent.act(agent.obs, agent.rnn_hxs, masks=agent.masks)\n",
    "env = agent.env\n",
    "next_obs, reward, done, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "68d59b72-58f6-4222-9d54-676c305d64ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean episode length 202.0, mean return 0.0\n",
      "Mean episode length 179.16666666666666, mean return 0.16666666666666666\n",
      "Mean episode length 202.0, mean return 0.0\n",
      "Mean episode length 202.0, mean return 0.0\n",
      "897 ms ± 22.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "agent.collect(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ef82f95f-5c3d-4e43-84af-06f1519ce831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean episode length 202.0, mean return 0.0\n",
      "Mean episode length 202.0, mean return 0.0\n",
      "Mean episode length 202.0, mean return 0.0\n",
      "6.85 ms ± 279 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "agent.collect(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "f09cd423-74af-4bad-ada2-3a60fa0599a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean episode length 202.0, mean return 0.0\n",
      "Mean episode length 202.0, mean return 0.0\n",
      "Mean episode length 194.25, mean return 0.08333333333333333\n",
      "3.05 ms ± 258 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "agent.collect(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "6b25c92c-5866-41c4-a14f-7b847ff833c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.46 ms ± 738 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "agent.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "2018855b-473e-4dc3-b569-ddb01d8a633f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666.6666666666666"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000/12*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f38be8c-6c27-4985-8dc0-1aadc4ac81d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2 ms ± 156 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sample = agent.rb.sample(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0453a47-f4ce-4df4-b03f-8c049743a728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511 µs ± 12.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sample = agent.rb.sample(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f880d22d-ca01-4994-a1e3-4037a68e0db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "04c414a1-1c5a-48d6-9a2b-5d0eaf8b98e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = agent.rb.sample(agent.batch_size//agent.sequence_length)\n",
    "states = sample['observations']\n",
    "next_states = sample['next_observations']\n",
    "hidden_states = sample['hidden_states']\n",
    "next_hidden_states = sample['next_hidden_states']\n",
    "actions = sample['actions']\n",
    "rewards = sample['rewards']\n",
    "dones = sample['dones']\n",
    "next_dones = sample['next_dones']\n",
    "#training_masks are given by SequenceReplayBuffer\n",
    "training_masks = sample['training_masks']\n",
    "\n",
    "with torch.no_grad():\n",
    "    target_q, _, _ = agent.target_network(next_states, next_hidden_states, next_dones)\n",
    "    target_max, _ = target_q.max(dim=2)\n",
    "    td_target = rewards + agent.gamma * target_max * (1 - dones)\n",
    "old_q, _, _ = agent.q_network(states, hidden_states, dones)\n",
    "old_val = old_q.gather(2, actions.long()).squeeze()\n",
    "\n",
    "# loss = F.mse_loss(td_target[:, agent.burn_in_length:], old_val[:, agent.burn_in_length:])\n",
    "weights = sample['weights']\n",
    "elementwise_loss = F.smooth_l1_loss(td_target[:, agent.burn_in_length:],\n",
    "                                    old_val[:, agent.burn_in_length:], reduction='none')\n",
    "# loss = torch.mean(elementwise_loss * weights)\n",
    "loss = torch.mean(elementwise_loss * weights * training_masks)\n",
    "\n",
    "if agent.writer is not None and agent.global_update_step % 10 == 0:\n",
    "    agent.writer.add_scalar('losses/td_loss', loss, agent.global_step)\n",
    "    agent.writer.add_scalar('losses/q_values', old_val.mean().item(), agent.global_step)\n",
    "    sps = int(agent.global_step / (time.time() - agent.start_time))\n",
    "    # print('SPS:', int(sps))\n",
    "    agent.writer.add_scalar('charts/SPS', sps, agent.global_step)\n",
    "\n",
    "agent.optimizer.zero_grad()\n",
    "loss.backward()\n",
    "agent.optimizer.step()\n",
    "\n",
    "# PER: update priorities\n",
    "td_priorities = elementwise_loss.mean(dim=1).detach().cpu().numpy() + 1e-6\n",
    "agent.rb.update_priorities(sample['idxs'], td_priorities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "d6946c26-960c-4035-b52c-dbf44026b997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "17ee9b38-e880-421c-8a4e-dc677752027b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "5f3e645b-c866-4453-8d26-26da7a243fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.collect(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "6a0c9ab5-9936-4d13-b9fa-7bcb06528867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.rb.cur_dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "07ed2497-cd9a-45d7-92e5-71c740c29b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.16666667, 0.16666667, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.16666667, 0.16666667, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.rb.cur_observations[:, :, 5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "64792cdc-f453-46ff-b4f7-2bb1cd38ba32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  8, 12, 12])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.rb.cur_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "c91aaca0-dd06-4852-bcdb-ba4e0d0a28a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.rb.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "f4399d54-5baf-4a3c-b1f5-1a2d57ccb5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.33333334, 0.33333334],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.rb.observations[360][:, 5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "f527ed18-5a1f-4278-a030-795d89e1128e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.rb.dones[367]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "7139b725-18d5-4f93-9fca-69fa2bb461fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.rb.training_masks[547]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "ec5c03cf-d0b2-4574-92b3-c91e7a523e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.rb.burn_in_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "16d92350-5782-4920-97e6-f395fbbc5fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.8333, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3611, 0.3736, 0.3897, 0.4103, 0.4364, 0.4024,\n",
       "         0.3713, 0.3468, 0.3275, 0.3123, 0.3005, 0.2914],\n",
       "        [0.5000, 0.5000, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3974, 0.4200, 0.4253, 0.3890, 0.3608, 0.3386,\n",
       "         0.3210, 0.3072, 0.2965, 0.2885, 0.2828, 0.2792],\n",
       "        [0.5000, 0.5000, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3974, 0.4200, 0.4253, 0.3890, 0.3608, 0.3386,\n",
       "         0.3210, 0.3072, 0.2965, 0.2885, 0.2828, 0.2792],\n",
       "        [0.5000, 0.5000, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3835, 0.4054, 0.3941, 0.3605, 0.3343, 0.3137,\n",
       "         0.2975, 0.2847, 0.2748, 0.2673, 0.2621, 0.2587],\n",
       "        [0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3796, 0.3492, 0.3255, 0.3067, 0.2920, 0.2804,\n",
       "         0.2715, 0.2650, 0.2605, 0.2579, 0.2572, 0.2582],\n",
       "        [0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3466, 0.3189, 0.2972, 0.2801, 0.2666, 0.2561,\n",
       "         0.2479, 0.2420, 0.2379, 0.2355, 0.2349, 0.2358],\n",
       "        [0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3137, 0.2886, 0.2690, 0.2535, 0.2413, 0.2317,\n",
       "         0.2244, 0.2190, 0.2153, 0.2132, 0.2125, 0.2134],\n",
       "        [0.5000, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3659, 0.3619, 0.3257, 0.2979, 0.2763, 0.2592,\n",
       "         0.2458, 0.2352, 0.2271, 0.2209, 0.2166, 0.2138],\n",
       "        [0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3137, 0.2886, 0.2690, 0.2535, 0.2413, 0.2317,\n",
       "         0.2244, 0.2190, 0.2153, 0.2132, 0.2125, 0.2134],\n",
       "        [0.5000, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3659, 0.3619, 0.3257, 0.2979, 0.2763, 0.2592,\n",
       "         0.2458, 0.2352, 0.2271, 0.2209, 0.2166, 0.2138],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.8333, 0.8333, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3325, 0.3440, 0.3588, 0.3778, 0.3390, 0.3082,\n",
       "         0.2843, 0.2656, 0.2508, 0.2392, 0.2301, 0.2231],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3163, 0.3213, 0.3288, 0.3390, 0.3524, 0.3696,\n",
       "         0.3538, 0.3195, 0.2931, 0.2725, 0.2563, 0.2435],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3135, 0.3131, 0.3149, 0.3190, 0.3254, 0.3345,\n",
       "         0.3466, 0.3623, 0.3704, 0.3321, 0.3029, 0.2802],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.8333, 0.8333, 0.8333,\n",
       "         0.8333, 0.8333, 0.8333, 0.3163, 0.3213, 0.3288, 0.3390, 0.3524, 0.3696,\n",
       "         0.3538, 0.3195, 0.2931, 0.2725, 0.2563, 0.2435],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bf9c2f7d-83a4-4608-b688-ae4afa8b8c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.9684e-04, 3.5871e-04, 3.9544e-04, 6.3365e-07, 4.3067e-04, 4.5668e-04,\n",
       "         4.9324e-04, 3.5182e-08],\n",
       "        [3.0544e-03, 3.2638e-03, 1.2259e-04, 1.1976e-04, 3.3821e-03, 5.0191e-04,\n",
       "         3.3470e-03, 6.1449e-07],\n",
       "        [1.9314e-04, 1.4818e-04, 7.3708e-04, 1.5522e-07, 6.0924e-03, 5.0173e-03,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [7.9361e-04, 3.8188e-07, 5.1485e-03, 5.1389e-03, 7.6737e-07, 4.6772e-03,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [9.4599e-05, 8.6761e-05, 1.0171e-04, 1.2294e-04, 8.1208e-04, 4.8006e-03,\n",
       "         0.0000e+00, 0.0000e+00]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_masks * elementwise_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4610297e-f4cd-457b-8132-f9a5f986ac94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.9684e-04, 3.5871e-04, 3.9544e-04, 6.3365e-07, 4.3067e-04, 4.5668e-04,\n",
       "         4.9324e-04, 3.5182e-08],\n",
       "        [3.0544e-03, 3.2638e-03, 1.2259e-04, 1.1976e-04, 3.3821e-03, 5.0191e-04,\n",
       "         3.3470e-03, 6.1449e-07],\n",
       "        [1.9314e-04, 1.4818e-04, 7.3708e-04, 1.5522e-07, 6.0924e-03, 5.0173e-03,\n",
       "         1.4288e-04, 1.4209e-04],\n",
       "        [7.9361e-04, 3.8188e-07, 5.1485e-03, 5.1389e-03, 7.6737e-07, 4.6772e-03,\n",
       "         1.7489e-04, 1.5157e-04],\n",
       "        [9.4599e-05, 8.6761e-05, 1.0171e-04, 1.2294e-04, 8.1208e-04, 4.8006e-03,\n",
       "         1.3216e-04, 1.3028e-04]], grad_fn=<SmoothL1LossBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elementwise_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "660802c1-6f40-4255-9fdd-cd9dab129042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample['idxs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "76221d10-5cc9-4a00-86e3-f4f41a4643de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123, 19, 16, 46, 43, 14, 93, 73, 30, 127, 123, 68, 122, 39, 13, 84]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['idxs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "71ea89c9-b939-4a7d-9f1e-eac1f5e3d6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.rb.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed5f91f4-53a1-4dfb-99ce-fdc56d3a8f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.018921368084995137, 1.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = agent.rb.sum_tree.capacity\n",
    "st = agent.rb.sum_tree\n",
    "st.tree[cap+126:130+cap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e143cdd-9f6e-451a-ae7a-7edef6cbe828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elementwise_loss.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39cd8d7b-e82f-4117-af4e-802c5980f9fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SequenceReplayBuffer' object has no attribute 'update_priorities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_priorities\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SequenceReplayBuffer' object has no attribute 'update_priorities'"
     ]
    }
   ],
   "source": [
    "agent.rb.update_priorities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
